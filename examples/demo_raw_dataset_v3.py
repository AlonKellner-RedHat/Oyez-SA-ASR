#!/usr/bin/env python3
# Generated by Claude
"""Demo: Loading oyez-sa-asr-raw dataset with HuggingFace datasets v3.x.

This demo uses trust_remote_code=True to load via the custom loading script,
which provides automatic audio decoding and embedded metadata (transcript
title, case name).

Requires: datasets>=3.0,<4.0

Usage:
    python examples/demo_raw_dataset_v3.py
"""

import argparse
from pathlib import Path


def check_version() -> bool:
    """Check if we're running datasets v3.x."""
    try:
        from oyez_sa_asr.hf_compat import (  # noqa: PLC0415
            is_v4,
            supports_loading_scripts,
        )

        if is_v4():
            print(
                "WARNING: You're running datasets v4.x which doesn't support loading scripts."
            )
            print(
                "This demo requires datasets v3.x. Use demo_raw_dataset.py for v4.x instead."
            )
            return False
        print(f"datasets supports loading scripts: {supports_loading_scripts()}")
        return True
    except ImportError:
        print("Note: oyez_sa_asr not installed, skipping version check")
        return True


def main() -> None:
    """Demo the raw dataset with v3.x trust_remote_code loading."""
    parser = argparse.ArgumentParser(description="Demo the raw dataset (v3.x)")
    parser.add_argument("--dataset-dir", type=Path, default=Path("datasets/raw"))
    parser.add_argument("--limit", type=int, default=3)
    args = parser.parse_args()

    print("=" * 60)
    print("Oyez SA-ASR Raw Dataset Demo (v3.x)")
    print("=" * 60)

    if not check_version():
        return

    if not args.dataset_dir.exists():
        print(f"\nError: Dataset not found at {args.dataset_dir}")
        print("Run the pipeline first: oyez pipeline run --term 2024")
        return

    try:
        from datasets import load_dataset  # noqa: PLC0415
    except ImportError:
        print("Error: 'datasets' package required. Install with: uv add datasets")
        return

    print(f"\nLoading from: {args.dataset_dir}")
    print("Using trust_remote_code=True for rich metadata access...")

    # Load via custom loading script (v3.x only)
    # The script path approach works for testing locally
    ds = load_dataset(
        "src/oyez_sa_asr/hf_scripts/raw.py",
        data_dir=str(args.dataset_dir),
        trust_remote_code=True,
        streaming=True,
    )

    print(f"\n{'=' * 60}")
    print(f"Sample Recordings (first {args.limit})")
    print("=" * 60)

    for i, sample in enumerate(ds["train"]):
        if i >= args.limit:
            break
        print(f"\n[{i + 1}] {sample.get('term', '?')}/{sample.get('docket', '?')}")
        print(f"    Recording ID: {sample.get('recording_id', 'N/A')}")
        print(f"    Title: {sample.get('title', 'N/A')[:60]}...")
        print(f"    Case: {sample.get('case_name', 'N/A')[:60]}...")

        # Audio is automatically decoded by the loading script
        audio = sample.get("audio")
        if audio:
            duration = len(audio["array"]) / audio["sampling_rate"]
            print(f"    Audio: {duration:.1f}s @ {audio['sampling_rate']} Hz")

    print("\n" + "=" * 60)
    print("v3.x Advantages:")
    print("  - Automatic audio decoding (no manual file loading)")
    print("  - Embedded metadata (title, case_name from JSON files)")
    print("  - Custom processing in loading script")
    print("=" * 60)


if __name__ == "__main__":
    main()
