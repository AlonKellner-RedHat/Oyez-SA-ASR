#!/usr/bin/env python3
# Generated by Claude
"""Demo: Loading oyez-sa-asr-simple dataset with HuggingFace datasets v3.x.

The simple dataset works identically in both v3.x and v4.x since it uses
parquet files with embedded audio (no custom loading script needed).

This demo shows the v3.x-style loading with trust_remote_code=True,
which is optional for this dataset.

Requires: datasets>=3.0,<4.0

Usage:
    python examples/demo_simple_dataset_v3.py
    python examples/demo_simple_dataset_v3.py --split lt5m
"""

import argparse
from pathlib import Path


def check_version() -> None:
    """Print version information."""
    try:
        from oyez_sa_asr.hf_compat import (  # noqa: PLC0415
            datasets_version,
            supports_loading_scripts,
        )

        print(f"datasets version: {datasets_version()}")
        print(f"Supports loading scripts: {supports_loading_scripts()}")
    except ImportError:
        print("Note: oyez_sa_asr not installed, skipping version check")


def main() -> None:
    """Demo the simple dataset with v3.x style loading."""
    parser = argparse.ArgumentParser(description="Demo the simple dataset (v3.x)")
    parser.add_argument("--dataset-dir", type=Path, default=Path("datasets/simple"))
    parser.add_argument("--split", default="lt1m", choices=["lt1m", "lt5m", "lt30m"])
    parser.add_argument("--limit", type=int, default=5)
    args = parser.parse_args()

    print("=" * 60)
    print("Oyez SA-ASR Simple Dataset Demo (v3.x)")
    print("=" * 60)

    check_version()

    if not args.dataset_dir.exists():
        print(f"\nError: Dataset not found at {args.dataset_dir}")
        print("Run the pipeline first: oyez pipeline run --term 2024")
        return

    try:
        from datasets import load_dataset  # noqa: PLC0415
    except ImportError:
        print("Error: 'datasets' package required. Install with: uv add datasets")
        return

    print(f"\nDataset directory: {args.dataset_dir}")
    print(f"Split: {args.split}")
    print("\nNote: trust_remote_code=True is optional for the simple dataset")
    print("since it uses parquet auto-discovery with embedded audio.")

    # Load with trust_remote_code (works in both v3.x and v4.x)
    ds = load_dataset(
        str(args.dataset_dir),
        args.split,
        trust_remote_code=True,  # Optional for simple dataset
    )

    print(f"\nLoaded {len(ds['train']):,} utterances")

    print(f"\n{'=' * 60}")
    print(f"Sample Utterances (first {args.limit})")
    print("=" * 60)

    for i, sample in enumerate(ds["train"]):
        if i >= args.limit:
            break
        print(f"\n[{i + 1}] {sample.get('term', '?')}/{sample.get('docket', '?')}")
        print(f"    Speaker: {sample.get('speaker', 'Unknown')}")
        print(f"    Duration: {sample.get('duration', 0):.2f}s")
        sentence = sample.get("sentence", "")[:60]
        print(f"    Sentence: {sentence}...")

        audio = sample.get("audio")
        if audio:
            actual_dur = len(audio["array"]) / audio["sampling_rate"]
            print(f"    Audio: {actual_dur:.1f}s @ {audio['sampling_rate']} Hz")

    # Calculate total duration
    total_dur = sum(s.get("duration", 0) or 0 for s in ds["train"])
    print(f"\nTotal duration: {total_dur / 3600:.1f} hours")

    print("\n" + "=" * 60)
    print("Note on v3.x vs v4.x:")
    print("  The simple dataset works identically in both versions")
    print("  since audio is embedded directly in parquet files.")
    print("  No custom loading script is needed.")
    print("=" * 60)


if __name__ == "__main__":
    main()
