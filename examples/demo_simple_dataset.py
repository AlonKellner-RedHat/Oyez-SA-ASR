#!/usr/bin/env python3
# Generated by Claude
"""Demo: Loading and using the oyez-sa-asr-simple dataset (v4.x compatible).

This demo loads data directly from parquet files using pyarrow.
Compatible with HuggingFace datasets v4.x (parquet auto-discovery).

The simple dataset works identically in v3.x and v4.x since audio is
embedded in parquet. See demo_simple_dataset_v3.py for HuggingFace
datasets loading with trust_remote_code=True.

This dataset has audio embedded directly in parquet files, making it
ideal for streaming and quick experimentation without managing files.

Usage:
    python examples/demo_simple_dataset.py
    python examples/demo_simple_dataset.py --split lt5m
    python examples/demo_simple_dataset.py --dataset-dir datasets/simple
"""

import argparse
from pathlib import Path

try:
    import pyarrow.parquet as pq

    HAS_PYARROW = True
except ImportError:
    HAS_PYARROW = False
    pq = None  # type: ignore[assignment]


def load_simple_dataset(dataset_dir: Path, split: str = "lt1m") -> list[dict]:
    """Load utterances from the simple dataset.

    Args:
        dataset_dir: Path to the simple dataset directory
        split: One of 'lt1m', 'lt5m', 'lt30m'

    Returns
    -------
        List of utterance dictionaries
    """
    if not HAS_PYARROW or pq is None:
        print("Error: pyarrow is required. Install with: uv add pyarrow")
        return []
    utterances = []
    split_dir = dataset_dir / split / "data" / "utterances"
    if not split_dir.exists():
        print(f"Error: Split directory not found: {split_dir}")
        return []
    for shard_file in sorted(split_dir.glob("*.parquet")):
        table = pq.read_table(shard_file)
        utterances.extend(table.to_pylist())
    return utterances


def print_samples(utterances: list[dict], limit: int) -> None:
    """Print sample utterances."""
    print(f"\n{'=' * 60}")
    print(f"Sample Utterances (first {limit})")
    print("=" * 60)
    for i, utt in enumerate(utterances[:limit]):
        print(f"\n[{i + 1}] {utt.get('term', '?')}/{utt.get('docket', '?')}")
        print(f"    Speaker: {utt.get('speaker', 'Unknown')}")
        print(f"    Duration: {utt.get('duration', 0):.2f}s")
        sentence = utt.get("sentence", "")[:80]
        print(f"    Sentence: {sentence}...")
        audio = utt.get("audio")
        if audio:
            # Audio is a dict with 'bytes' and 'path' keys
            audio_bytes = audio.get("bytes") if isinstance(audio, dict) else audio
            if audio_bytes:
                print(f"    Audio: {len(audio_bytes):,} bytes")


def main() -> None:
    """Demo the simple dataset with embedded audio."""
    parser = argparse.ArgumentParser(description="Demo the simple dataset")
    parser.add_argument("--dataset-dir", type=Path, default=Path("datasets/simple"))
    parser.add_argument("--split", default="lt1m", choices=["lt1m", "lt5m", "lt30m"])
    parser.add_argument("--limit", type=int, default=5)
    args = parser.parse_args()

    print("=" * 60)
    print("Oyez SA-ASR Simple Dataset Demo")
    print("=" * 60)
    print(f"\nDataset directory: {args.dataset_dir}")
    print(f"Split: {args.split}")

    if not args.dataset_dir.exists():
        print(f"\nError: Dataset not found at {args.dataset_dir}")
        print("Run the pipeline first: oyez pipeline run --term 2024")
        return

    utterances = load_simple_dataset(args.dataset_dir, args.split)
    print(f"Loaded {len(utterances):,} utterances")

    if utterances:
        print_samples(utterances, args.limit)
        total_dur = sum(u.get("duration", 0) or 0 for u in utterances)
        print(f"\nTotal duration: {total_dur / 3600:.1f} hours")

    print("\n" + "=" * 60)
    print("Demo complete!")


if __name__ == "__main__":
    main()
