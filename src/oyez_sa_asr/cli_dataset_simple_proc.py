# Generated by Claude
"""Processing helpers for dataset simple command.

Contains the parallel processing logic for embedding audio segments.
"""

import logging
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import Any

from tqdm import tqdm

from .audio_segment import extract_segments_batch

logger = logging.getLogger(__name__)


def group_utterances_by_recording(
    utterances: list[dict[str, Any]],
) -> dict[tuple[str, str], list[dict[str, Any]]]:
    """Group utterances by recording (term, docket).

    Returns
    -------
        Dictionary mapping (term, docket) to list of utterances.
    """
    grouped: dict[tuple[str, str], list[dict[str, Any]]] = defaultdict(list)
    for utt in utterances:
        key = (utt["term"], utt["docket"])
        grouped[key].append(utt)
    return dict(grouped)


def process_single_recording(
    args: tuple[tuple[str, str], list[dict[str, Any]], Path],
) -> tuple[list[dict[str, Any]], int]:
    """Process a single recording and return embedded rows.

    This function is designed to be called in parallel processes.

    Args:
        args: Tuple of (key, utterances, audio_path) where key is (term, docket).

    Returns
    -------
        Tuple of (list of row dicts, error_count).
    """
    key, rec_utterances, audio_path = args

    # Filter out utterances with missing or invalid time ranges
    valid_utterances = []
    segments = []
    for utt in rec_utterances:
        start = utt.get("start_sec")
        end = utt.get("end_sec")
        if start is None or end is None or start >= end:
            continue  # Skip invalid utterances
        valid_utterances.append(utt)
        segments.append((start, end))

    if not segments:
        return [], 0

    rec_utterances = valid_utterances

    try:
        segment_bytes_list = extract_segments_batch(audio_path, segments)
    except (OSError, ValueError) as e:
        logger.warning("Failed to process %s: %s", audio_path, e)
        return [], len(rec_utterances)

    rows = []
    for utt, audio_bytes in zip(rec_utterances, segment_bytes_list, strict=True):
        term = utt.get("term", key[0])
        docket = utt.get("docket", key[1])
        segment_name = f"{term}_{docket}_{utt.get('start_sec', 0):.1f}.flac"
        row = {
            "audio": {"bytes": audio_bytes, "path": segment_name},
            "text": utt.get("text", ""),
            "speaker_name": utt.get("speaker_name"),
            "term": term,
            "docket": docket,
            "start_sec": utt.get("start_sec"),
            "end_sec": utt.get("end_sec"),
        }
        rows.append(row)

    return rows, 0


def process_by_recording(
    utterances: list[dict[str, Any]],
    audio_paths: dict[tuple[str, str], Path],
    output_dir: Path,
    shard_size_mb: int,
    pa: Any,
    pq: Any,
    workers: int = 1,
) -> dict[str, int]:
    """Process utterances grouped by recording for efficiency.

    Reads each audio file ONCE and extracts all segments from it,
    avoiding redundant file reads. Uses parallel processing for speed.

    Note
    ----
        Shard contents are non-deterministic due to parallel processing
        completion order. This is acceptable for ML training data.
    """
    data_dir = output_dir / "data" / "utterances"
    data_dir.mkdir(parents=True, exist_ok=True)
    target_bytes = shard_size_mb * 1024 * 1024

    # Group utterances by recording
    grouped = group_utterances_by_recording(utterances)

    # Build work items for parallel processing
    work_items = []
    skipped_count = 0
    for key, rec_utterances in grouped.items():
        audio_path = audio_paths.get(key)
        if audio_path is None or not audio_path.exists():
            skipped_count += len(rec_utterances)
            continue
        work_items.append((key, rec_utterances, audio_path))

    current_shard: list[dict[str, Any]] = []
    current_size = 0
    shard_num = 0
    embedded_count = 0
    error_count = 0

    # Process recordings in parallel.
    # Note: as_completed() returns futures in completion order, not submission
    # order, so shard contents will vary between runs. This is acceptable for
    # ML training data where order doesn't matter.
    with ProcessPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(process_single_recording, item): item for item in work_items
        }

        with tqdm(total=len(futures), desc="Recordings", unit="rec") as pbar:
            for future in as_completed(futures):
                rows, errors = future.result()
                error_count += errors

                for row in rows:
                    current_shard.append(row)
                    current_size += len(row["audio"]["bytes"])
                    embedded_count += 1

                    # Write shard when target size reached
                    if current_size >= target_bytes:
                        pq.write_table(
                            pa.Table.from_pylist(current_shard),
                            data_dir / f"train-{shard_num:05d}.parquet",
                        )
                        shard_num += 1
                        current_shard = []
                        current_size = 0

                pbar.update(1)

    # Write final shard
    if current_shard:
        pq.write_table(
            pa.Table.from_pylist(current_shard),
            data_dir / f"train-{shard_num:05d}.parquet",
        )
        shard_num += 1

    return {
        "embedded": embedded_count,
        "skipped": skipped_count,
        "errors": error_count,
        "shards": shard_num,
    }
