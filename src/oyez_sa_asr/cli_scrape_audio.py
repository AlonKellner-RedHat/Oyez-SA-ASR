# Generated by Claude
"""Scrape audio subcommand for oyez_sa_asr CLI."""

import asyncio
from pathlib import Path
from typing import Annotated

import typer
from rich.console import Console
from tqdm import tqdm

from .scraper import AdaptiveFetcher, FetchResult, RequestMetadata
from .scraper.parser_transcripts import extract_audio_urls

console = Console(force_terminal=True)


def _is_expected_skip(result: FetchResult) -> bool:
    """Check if failure is expected (file missing or access denied).

    Expected skips:
    - 404/NoSuchKey: file doesn't exist in S3
    - 403/AccessDenied: file not publicly accessible
    - Unrecognized S3 URL: non-S3 URL (api.oyez.org legacy format)
    """
    err = str(result.error or "")
    return (
        result.status_code in (403, 404)
        or "NoSuchKey" in err
        or "AccessDenied" in err
        or "Unrecognized S3 URL" in err
    )


def add_audio_command(app: typer.Typer) -> None:
    """Add the audio command to the scrape app."""

    @app.command(name="audio")
    def scrape_audio(
        transcripts_dir: Annotated[
            Path,
            typer.Option(
                "--transcripts-dir", "-t", help="Directory with processed transcripts"
            ),
        ] = Path("data/transcripts"),
        cache_dir: Annotated[
            Path,
            typer.Option("--cache-dir", "-c", help="Directory for caching audio files"),
        ] = Path(".cache/audio"),
        max_parallelism: Annotated[
            int,
            typer.Option("--max-parallelism", "-p", help="Maximum parallel downloads"),
        ] = 64,
        min_improvement: Annotated[
            float,
            typer.Option(
                "--min-improvement",
                "-m",
                help="Min rate improvement to scale (0.25=25%)",
            ),
        ] = 0.25,
    ) -> None:
        """Download audio files from S3 using adaptive parallelism.

        Extracts audio URLs from processed transcripts and downloads MP3, OGG,
        and HLS files to the cache directory.
        """
        console.print("[bold]Scraping audio files (adaptive S3 parallelism)[/bold]")
        console.print(f"  Transcripts dir: {transcripts_dir}")
        console.print(f"  Cache dir: {cache_dir}")
        console.print(f"  Max parallelism: {max_parallelism}")
        console.print(f"  Min improvement: {min_improvement:.0%}")
        console.print()

        # Extract audio URLs from processed transcripts
        audio_urls = extract_audio_urls(transcripts_dir)

        if not audio_urls:
            console.print(
                "[yellow]Warning:[/yellow] No audio URLs found in transcripts."
            )
            console.print(
                "Run 'process transcripts' first to generate transcript metadata."
            )
            return

        console.print(f"Found {len(audio_urls)} audio URLs to download")
        console.print()

        # Create requests from URLs
        requests = [RequestMetadata(url=url) for url in audio_urls]

        # Use S3 fetcher
        fetcher = AdaptiveFetcher.create_s3(
            cache_dir,
            max_parallelism=max_parallelism,
            min_improvement=min_improvement,
        )

        stats = {"new": 0, "skipped": 0, "error": 0}
        pbar: tqdm[None] | None = None

        def on_progress(
            completed: int, total: int, result: FetchResult, parallelism: int
        ) -> None:
            nonlocal pbar
            if pbar is None:
                pbar = tqdm(
                    total=total, desc="Downloading", unit="file", dynamic_ncols=True
                )

            if result.success:
                stats["new"] += 1
            elif _is_expected_skip(result):
                stats["skipped"] += 1  # Expected: not available
            else:
                stats["error"] += 1  # Unexpected error

            pbar.n = completed
            pbar.set_postfix(
                p=parallelism,
                ok=stats["new"],
                skip=stats["skipped"],
                err=stats["error"],
            )
            pbar.refresh()

        async def run_fetch() -> list[FetchResult]:
            return await fetcher.fetch_batch_adaptive(requests, on_progress)

        all_results = asyncio.run(run_fetch())
        if pbar is not None:
            pbar.close()

        cached = sum(1 for r in all_results if r.from_cache)
        new_downloads = sum(1 for r in all_results if r.success and not r.from_cache)
        skipped = sum(1 for r in all_results if not r.success and _is_expected_skip(r))
        errors = sum(
            1 for r in all_results if not r.success and not _is_expected_skip(r)
        )

        console.print()
        console.print("[bold green]Done![/bold green]")
        console.print(f"  Already cached: {cached}")
        console.print(f"  New downloads: {new_downloads}")
        console.print(f"  Skipped (unavailable): {skipped}")
        if errors > 0:
            console.print(f"  [red]Errors: {errors}[/red]")
        console.print(f"  Cache: {cache_dir}")
