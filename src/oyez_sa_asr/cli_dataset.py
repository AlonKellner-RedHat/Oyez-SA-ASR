# Generated by Claude
"""Dataset creation commands for HuggingFace datasets."""

import json
from pathlib import Path
from typing import Annotated

import typer
from rich.console import Console

from .cli_dataset_helpers import (
    collect_recordings,
    collect_utterances,
    copy_raw_audio,
    copy_raw_cases,
    copy_raw_transcripts,
    copy_tree,
    require_pyarrow,
)
from .cli_dataset_simple import register_simple_command
from .term_filter import filter_dirs

dataset_app = typer.Typer(help="Create HuggingFace datasets")
console = Console(force_terminal=True)

# Register the simple command from its module
register_simple_command(dataset_app)

# Re-export for tests
_collect_recordings = collect_recordings
_collect_utterances = collect_utterances
_copy_tree = copy_tree


@dataset_app.command(name="raw")
def dataset_raw(
    cache_dir: Annotated[
        Path,
        typer.Option("--cache-dir", "-c", help="Root cache directory"),
    ] = Path(".cache"),
    output_dir: Annotated[
        Path,
        typer.Option("--output-dir", "-o", help="Dataset output directory"),
    ] = Path("datasets/raw"),
    terms: Annotated[
        list[str] | None,
        typer.Option("--term", "-T", help="Filter to specific term(s)"),
    ] = None,
) -> None:
    """Create oyez-sa-asr-raw dataset from cached files."""
    console.print("[bold]Creating oyez-sa-asr-raw dataset[/bold]")
    console.print(f"  Cache dir: {cache_dir}")
    console.print(f"  Output dir: {output_dir}")
    if terms:
        console.print(f"  Terms: {', '.join(terms)}")
    console.print()

    output_dir.mkdir(parents=True, exist_ok=True)
    term_set = set(terms) if terms else None

    total = copy_raw_audio(cache_dir, output_dir, terms)
    total += copy_raw_cases(cache_dir, output_dir, term_set)
    total += copy_raw_transcripts(cache_dir, output_dir, term_set)

    # Create index file
    index = {"terms": sorted(terms) if terms else [], "created_by": "oyez dataset raw"}
    with (output_dir / "index.json").open("w") as f:
        json.dump(index, f, indent=2)

    console.print()
    console.print(f"[bold green]Done![/bold green] Copied {total} files.")
    console.print(f"Output: {output_dir}")


@dataset_app.command(name="flex")
def dataset_flex(
    data_dir: Annotated[
        Path,
        typer.Option("--data-dir", "-d", help="Processed data directory"),
    ] = Path("data"),
    output_dir: Annotated[
        Path,
        typer.Option("--output-dir", "-o", help="Dataset output directory"),
    ] = Path("datasets/flex"),
    terms: Annotated[
        list[str] | None,
        typer.Option("--term", "-T", help="Filter to specific term(s)"),
    ] = None,
) -> None:
    """Create oyez-sa-asr-flex dataset with FLAC audio and parquet references."""
    pa, pq = require_pyarrow()

    console.print("[bold]Creating oyez-sa-asr-flex dataset[/bold]")
    console.print(f"  Data dir: {data_dir}")
    console.print(f"  Output dir: {output_dir}")
    if terms:
        console.print(f"  Terms: {', '.join(terms)}")
    console.print()

    output_dir.mkdir(parents=True, exist_ok=True)

    # Copy FLAC files
    audio_src = data_dir / "audio"
    audio_dst = output_dir / "audio"
    total_audio = 0
    if audio_src.exists():
        term_dirs = filter_dirs(list(audio_src.iterdir()), terms)
        for term_dir in term_dirs:
            if term_dir.is_dir():
                count = copy_tree(
                    term_dir, audio_dst / term_dir.name, f"Audio {term_dir.name}"
                )
                total_audio += count
    console.print(f"  Copied {total_audio} audio files")

    # Create recordings parquet
    parquet_dir = output_dir / "data"
    parquet_dir.mkdir(parents=True, exist_ok=True)

    console.print("Creating recordings.parquet...")
    recordings = collect_recordings(audio_src, terms)
    if recordings:
        table = pa.Table.from_pylist(recordings)
        pq.write_table(table, parquet_dir / "recordings.parquet")
        console.print(f"  {len(recordings)} recordings")

    # Create utterances parquet
    console.print("Creating utterances.parquet...")
    utterances = collect_utterances(data_dir / "transcripts", terms)
    if utterances:
        table = pa.Table.from_pylist(utterances)
        pq.write_table(table, parquet_dir / "utterances.parquet")
        console.print(f"  {len(utterances)} utterances")

    # Create index
    index = {
        "terms": sorted(terms) if terms else [],
        "recordings": len(recordings),
        "utterances": len(utterances),
        "created_by": "oyez dataset flex",
    }
    with (output_dir / "index.json").open("w") as f:
        json.dump(index, f, indent=2)

    console.print()
    console.print("[bold green]Done![/bold green]")
    console.print(f"Output: {output_dir}")
