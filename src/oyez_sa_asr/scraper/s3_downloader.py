# Generated by Claude
"""S3 downloader implementation using aiobotocore."""

import json
import re
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any

from aiobotocore.session import get_session
from botocore import UNSIGNED
from botocore.config import Config
from botocore.exceptions import ClientError

from .models import FetchResult, RequestMetadata

# Default status codes for files that are expected to be unavailable (not errors)
# 403: AccessDenied - file exists but not publicly accessible
# 404: NoSuchKey - file doesn't exist (common for HLS streams, older cases)
DEFAULT_EXPECTED_UNAVAILABLE_CODES: frozenset[int] = frozenset({403, 404})


def parse_s3_url(url: str) -> tuple[str, str]:
    """Parse S3 URL to extract bucket and key.

    Supports:
    - https://s3.amazonaws.com/bucket/key
    - https://bucket.s3.amazonaws.com/key

    Args:
        url: The S3 URL to parse.

    Returns
    -------
        Tuple of (bucket, key).

    Raises
    ------
        ValueError: If URL format is not recognized.
    """
    # Pattern: https://s3.amazonaws.com/bucket/key
    match = re.match(r"https://s3\.amazonaws\.com/([^/]+)/(.+)", url)
    if match:
        return match.group(1), match.group(2)

    # Pattern: https://bucket.s3.amazonaws.com/key
    match = re.match(r"https://([^.]+)\.s3\.amazonaws\.com/(.+)", url)
    if match:
        return match.group(1), match.group(2)

    msg = f"Unrecognized S3 URL format: {url}"
    raise ValueError(msg)


class S3Downloader:
    """S3 download backend using aiobotocore with anonymous access."""

    def __init__(
        self,
        cache_dir: Path,
        *,
        max_retries: int = 3,
        expected_unavailable_codes: frozenset[int] | None = None,
    ) -> None:
        """Initialize the S3 downloader.

        Args:
            cache_dir: Directory for caching downloaded files.
            max_retries: Maximum retry attempts for transient failures.
            expected_unavailable_codes: Status codes to cache as "unavailable" (not errors).
                Defaults to {403, 404} for S3 AccessDenied/NoSuchKey.
        """
        self.cache_dir = cache_dir
        self.max_retries = max_retries
        self.expected_unavailable_codes = (
            expected_unavailable_codes
            if expected_unavailable_codes is not None
            else DEFAULT_EXPECTED_UNAVAILABLE_CODES
        )
        self._session = get_session()

    def _get_local_path(self, bucket: str, key: str) -> Path:
        """Get local cache path for an S3 object."""
        return self.cache_dir / bucket / key

    def _get_unavailable_dir(self) -> Path:
        """Get directory for storing unavailable file metadata."""
        d = self.cache_dir / "_unavailable"
        d.mkdir(parents=True, exist_ok=True)
        return d

    def _get_unavailable_path(self, request: RequestMetadata) -> Path:
        """Get path for an unavailable file's cached metadata."""
        return self._get_unavailable_dir() / f"{request.cache_key()}.json"

    def _cache_unavailable(self, request: RequestMetadata, result: FetchResult) -> None:
        """Cache an expected unavailable result (403/404)."""
        path = self._get_unavailable_path(request)
        with path.open("w") as f:
            json.dump(
                {
                    "url": request.url,
                    "status_code": result.status_code,
                    "error": result.error,
                },
                f,
            )

    def _check_unavailable_cache(self, request: RequestMetadata) -> FetchResult | None:
        """Check if this file was previously found to be unavailable."""
        path = self._get_unavailable_path(request)
        if not path.exists():
            return None
        try:
            with path.open() as f:
                data = json.load(f)
            return FetchResult(
                url=request.url,
                success=False,
                status_code=data.get("status_code"),
                error=data.get("error"),
                from_cache=True,
            )
        except (json.JSONDecodeError, KeyError):
            path.unlink(missing_ok=True)
            return None

    def check_cache(self, request: RequestMetadata) -> FetchResult | None:
        """Check if file already exists in cache (success or permanent failure)."""
        try:
            bucket, key = parse_s3_url(request.url)
        except ValueError:
            return None

        # Check for successful download
        local_path = self._get_local_path(bucket, key)
        if local_path.exists():
            return FetchResult(
                url=request.url,
                success=True,
                status_code=200,
                data=str(local_path),
                content_type="application/octet-stream",
                from_cache=True,
            )

        # Check for cached unavailable (403/404)
        return self._check_unavailable_cache(request)

    def is_transient_failure(self, result: FetchResult) -> bool:
        """Check if failure is transient and should be retried."""
        if result.success:
            return False
        # S3 transient errors: throttling, service errors
        transient_codes = {429, 500, 502, 503, 504}
        return result.status_code in transient_codes or result.status_code is None

    async def fetch(self, client: Any, request: RequestMetadata) -> FetchResult:
        """Download file from S3 to local cache."""
        try:
            bucket, key = parse_s3_url(request.url)
        except ValueError as e:
            return FetchResult(
                url=request.url,
                success=False,
                error=str(e),
            )

        local_path = self._get_local_path(bucket, key)

        try:
            # Get object from S3
            response = await client.get_object(Bucket=bucket, Key=key)

            # Read body and write to file
            async with response["Body"] as stream:
                body = await stream.read()

            # Ensure directory exists
            local_path.parent.mkdir(parents=True, exist_ok=True)

            # Write to file
            local_path.write_bytes(body)

            return FetchResult(
                url=request.url,
                success=True,
                status_code=200,
                data=str(local_path),
                content_type=response.get("ContentType", "application/octet-stream"),
                from_cache=False,
            )

        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "Unknown")
            status_code = e.response.get("ResponseMetadata", {}).get(
                "HTTPStatusCode", None
            )
            result = FetchResult(
                url=request.url,
                success=False,
                status_code=status_code,
                error=f"S3 error {error_code}: {e}",
            )
            # Cache expected unavailable files so we don't retry them
            if status_code in self.expected_unavailable_codes:
                self._cache_unavailable(request, result)
            return result
        except Exception as e:
            return FetchResult(
                url=request.url,
                success=False,
                error=f"Download error: {e}",
            )

    @asynccontextmanager
    async def create_client(self) -> Any:
        """Create aiobotocore S3 client with anonymous access."""
        async with self._session.create_client(
            "s3",
            config=Config(signature_version=UNSIGNED),
        ) as client:
            yield client
