# Generated by Claude
"""Parser for cached Oyez transcript responses."""

import json
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any


def parse_transcript_type(title: str) -> tuple[str, str | None]:
    """Parse transcript title to extract type and speaker.

    Args:
        title: Transcript title from API.

    Returns
    -------
        Tuple of (transcript_type, speaker_name).
        transcript_type is one of: oral_argument, opinion, dissent, concurrence, unknown.
        speaker_name is None except for dissent/concurrence.
    """
    # Oral Argument
    if title.startswith("Oral Argument"):
        return ("oral_argument", None)

    # Dissenting Opinion - Justice - Date
    if title.startswith("Dissenting Opinion - "):
        match = re.match(r"Dissenting Opinion - (\w+) - ", title)
        if match:
            return ("dissent", match.group(1))

    # Concurring Opinion - Justice - Date
    if title.startswith("Concurring Opinion - "):
        match = re.match(r"Concurring Opinion - (\w+) - ", title)
        if match:
            return ("concurrence", match.group(1))

    # Opinion Announcement (including with Part N)
    if title.startswith("Opinion Announcement"):
        return ("opinion", None)

    return ("unknown", None)


# Validation thresholds
_MIN_WPM = 30  # Below this is suspicious (silence/timestamp error)
_MAX_WPM = 600  # Above this is impossible
_MAX_OVERLAP_SEC = 3.0  # Overlaps beyond this are invalid


def _validate_turn(
    duration: float,
    word_count: int,
    text: str,
    overlap_seconds: float,
) -> tuple[bool, str | None]:
    """Validate a turn and return (is_valid, invalid_reason)."""
    # Invalid timestamps
    if duration <= 0:
        return False, "invalid_duration"

    # Empty text
    if not text.strip():
        return False, "empty_text"

    # Abnormal WPM (only check for turns > 10 seconds)
    if duration > 10:
        wpm = word_count / (duration / 60) if duration > 0 else 0
        if wpm < _MIN_WPM:
            return False, f"wpm_too_low:{wpm:.1f}"
        if wpm > _MAX_WPM:
            return False, f"wpm_too_high:{wpm:.1f}"

    # Excessive overlap
    if overlap_seconds > _MAX_OVERLAP_SEC:
        return False, f"overlap:{overlap_seconds:.1f}s"

    return True, None


@dataclass
class ProcessedTurn:
    """A processed turn from a transcript."""

    index: int
    section_index: int
    start: float
    stop: float
    duration: float
    speaker_id: int | None
    speaker_name: str | None
    is_valid: bool
    invalid_reason: str | None
    is_overlapping: bool
    overlap_seconds: float
    text_block_count: int
    char_count: int
    word_count: int
    text: str

    @classmethod
    def from_raw(
        cls,
        raw: dict[str, Any],
        index: int,
        section_index: int,
        prev_stop: float | None = None,
    ) -> "ProcessedTurn":
        """Parse from raw API turn data."""
        start = raw.get("start", 0.0) or 0.0
        stop = raw.get("stop", 0.0) or 0.0
        duration = stop - start

        speaker = raw.get("speaker")
        speaker_id = speaker.get("ID") if speaker else None
        speaker_name = speaker.get("name") if speaker else None

        text_blocks = raw.get("text_blocks") or []
        texts = [block.get("text", "") for block in text_blocks]
        text = " ".join(t.strip() for t in texts if t.strip())

        char_count = len(text)
        word_count = len(text.split()) if text else 0

        # Calculate overlap
        overlap_seconds = 0.0
        if prev_stop is not None and start < prev_stop:
            overlap_seconds = prev_stop - start
        is_overlapping = overlap_seconds > 0

        # Validate turn
        is_valid, invalid_reason = _validate_turn(
            duration=duration,
            word_count=word_count,
            text=text,
            overlap_seconds=overlap_seconds,
        )

        return cls(
            index=index,
            section_index=section_index,
            start=start,
            stop=stop,
            duration=duration,
            speaker_id=speaker_id,
            speaker_name=speaker_name,
            is_valid=is_valid,
            invalid_reason=invalid_reason,
            is_overlapping=is_overlapping,
            overlap_seconds=overlap_seconds,
            text_block_count=len(text_blocks),
            char_count=char_count,
            word_count=word_count,
            text=text,
        )

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "index": self.index,
            "section_index": self.section_index,
            "start": self.start,
            "stop": self.stop,
            "duration": self.duration,
            "speaker_id": self.speaker_id,
            "speaker_name": self.speaker_name,
            "is_valid": self.is_valid,
            "invalid_reason": self.invalid_reason,
            "is_overlapping": self.is_overlapping,
            "overlap_seconds": self.overlap_seconds,
            "text_block_count": self.text_block_count,
            "char_count": self.char_count,
            "word_count": self.word_count,
            "text": self.text,
        }


def build_transcript_to_case_map(
    cases_dir: Path, terms: list[str] | None = None
) -> dict[int, tuple[str, str]]:
    """Build a mapping from transcript IDs to case info.

    Args:
        cases_dir: Directory containing processed case files.
        terms: Optional list of terms to filter by.

    Returns
    -------
        Dict mapping transcript ID to (term, docket_number).
    """
    case_map: dict[int, tuple[str, str]] = {}

    if not cases_dir.exists():
        return case_map

    term_set = set(terms) if terms else None

    for term_dir in cases_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for case_file in term_dir.glob("*.json"):
            try:
                with case_file.open() as f:
                    case_data = json.load(f)

                term = case_data.get("term", "")
                docket = case_data.get("docket_number", "")

                for audio in case_data.get("oral_arguments", []) or []:
                    if audio and audio.get("id"):
                        case_map[audio["id"]] = (term, docket)

                for audio in case_data.get("opinion_announcements", []) or []:
                    if audio and audio.get("id"):
                        case_map[audio["id"]] = (term, docket)

            except (json.JSONDecodeError, KeyError, TypeError):
                continue

    return case_map


def extract_audio_urls(
    transcripts_dir: Path, terms: list[str] | None = None
) -> list[str]:
    """Extract all audio URLs from processed transcripts.

    Args:
        transcripts_dir: Directory containing processed transcript files.
        terms: Optional list of terms to filter by.

    Returns
    -------
        List of unique audio URLs (mp3, ogg, hls).
    """
    audio_urls: set[str] = set()

    if not transcripts_dir.exists():
        return []

    term_set = set(terms) if terms else None

    for term_dir in transcripts_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            for transcript_file in docket_dir.glob("*.json"):
                try:
                    with transcript_file.open() as f:
                        transcript_data = json.load(f)

                    audio_data = transcript_data.get("metadata", {}).get(
                        "audio_urls", {}
                    )
                    for url in audio_data.values():
                        if url:
                            audio_urls.add(url)

                except (json.JSONDecodeError, KeyError, TypeError):
                    continue

    return sorted(audio_urls)


# Re-export ProcessedTranscript from transcript_models to maintain backwards compat
from .transcript_models import ProcessedTranscript  # noqa: E402, F401
