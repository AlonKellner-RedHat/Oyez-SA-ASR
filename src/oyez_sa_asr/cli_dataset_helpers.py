# Generated by Claude
"""Helper functions for dataset creation commands."""

import json
import shutil
from pathlib import Path
from typing import Any

import typer
from rich.console import Console
from tqdm import tqdm

from .audio_source import parse_transcript_type_from_recording_id
from .term_filter import filter_dirs

console = Console(force_terminal=True)


def load_justice_speaker_ids(speakers_dir: Path | None = None) -> set[int]:
    """Load set of justice speaker IDs from speaker files.

    Edited by Claude: Helper to identify justice speakers for metadata.

    Args:
        speakers_dir: Directory containing speakers/justices/ subdirectory.
                     Defaults to data/speakers if None.

    Returns
    -------
        Set of speaker IDs who are justices.
    """
    if speakers_dir is None:
        speakers_dir = Path("data/speakers")

    justices_dir = speakers_dir / "justices"
    if not justices_dir.exists():
        return set()

    justice_ids: set[int] = set()
    for speaker_file in justices_dir.glob("*.json"):
        try:
            with speaker_file.open() as f:
                data = json.load(f)
            speaker_id = data.get("id")
            if speaker_id is not None:
                justice_ids.add(speaker_id)
        except (json.JSONDecodeError, KeyError):
            continue

    return justice_ids


def require_pyarrow() -> tuple[Any, Any]:
    """Import and return pyarrow modules, or exit if not installed."""
    try:
        import pyarrow as pa_mod  # noqa: PLC0415
        import pyarrow.parquet as pq_mod  # noqa: PLC0415

        return pa_mod, pq_mod
    except ImportError:
        console.print("[red]Error:[/red] pyarrow not installed.")
        console.print("Run: uv add pyarrow")
        raise typer.Exit(1) from None


def copy_tree(src: Path, dst: Path, desc: str = "Copying") -> int:
    """Copy a directory tree, returning the number of files copied."""
    if not src.exists():
        return 0
    count = 0
    files = [f for f in src.rglob("*") if f.is_file()]
    for file in tqdm(files, desc=desc, unit="file"):
        dest = dst / file.relative_to(src)
        dest.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(file, dest)
        count += 1
    return count


def copy_raw_audio(cache_dir: Path, output_dir: Path, terms: list[str] | None) -> int:
    """Copy raw audio files from cache."""
    total = 0
    for fmt in ("mp3", "ogg"):
        audio_cache = cache_dir / "audio" / f"oyez.case-media.{fmt}" / "case_data"
        if not audio_cache.exists():
            continue
        for term_dir in filter_dirs(list(audio_cache.iterdir()), terms):
            if term_dir.is_dir():
                dst = output_dir / "audio" / term_dir.name
                total += copy_tree(term_dir, dst, f"Audio {fmt}/{term_dir.name}")
    return total


def matches_term(json_file: Path, term_set: set[str]) -> bool:
    """Check if a JSON file's term is in the term set."""
    try:
        with json_file.open() as f:
            data = json.load(f)
        return data.get("term") in term_set
    except (json.JSONDecodeError, KeyError):
        return False


def copy_raw_cases(cache_dir: Path, output_dir: Path, term_set: set[str] | None) -> int:
    """Copy raw case JSON files from cache."""
    cases_cache = cache_dir / "cases" / "api.oyez.org" / "raw"
    if not cases_cache.exists():
        return 0
    cases_out = output_dir / "cases"
    cases_out.mkdir(parents=True, exist_ok=True)
    count = 0
    for json_file in tqdm(list(cases_cache.glob("*.json")), desc="Cases", unit="file"):
        if term_set and not matches_term(json_file, term_set):
            continue
        shutil.copy2(json_file, cases_out / json_file.name)
        count += 1
    return count


def copy_raw_transcripts(
    cache_dir: Path, output_dir: Path, term_set: set[str] | None
) -> int:
    """Copy raw transcript JSON files from cache.

    Note: Raw transcripts are stored by media ID, not term. Term filtering
    requires parsing each file to extract term info, which is not currently
    supported. When term filtering is requested, all transcripts are copied.
    """
    if term_set:
        console.print(
            "  [yellow]Note:[/yellow] Transcript term filtering not supported; "
            "copying all transcripts"
        )
    transcripts_cache = cache_dir / "transcripts" / "api.oyez.org" / "raw"
    if not transcripts_cache.exists():
        return 0
    transcripts_out = output_dir / "transcripts"
    transcripts_out.mkdir(parents=True, exist_ok=True)
    count = 0
    for json_file in tqdm(
        list(transcripts_cache.glob("*.json")), desc="Transcripts", unit="file"
    ):
        shutil.copy2(json_file, transcripts_out / json_file.name)
        count += 1
    return count


def collect_raw_recordings(
    audio_dir: Path, terms: list[str] | None
) -> list[dict[str, str | None]]:
    """Collect raw recording metadata from MP3/OGG files for HuggingFace parquet.

    Creates a parquet with audio paths that HuggingFace can resolve.
    """
    records: list[dict[str, str | None]] = []
    term_set = set(terms) if terms else None

    if not audio_dir.exists():
        return records

    # Track unique recording IDs
    seen: set[str] = set()

    for term_dir in audio_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            # Collect both MP3 and OGG files
            for audio_file in docket_dir.glob("*"):
                if audio_file.suffix not in (".mp3", ".ogg"):
                    continue

                # Extract recording ID (handle .delivery.mp3 pattern)
                rec_id = audio_file.stem.split(".")[0]
                if rec_id in seen:
                    continue
                seen.add(rec_id)

                # Find best audio file (prefer MP3)
                mp3_file = next(docket_dir.glob(f"{rec_id}*.mp3"), None)
                ogg_file = next(docket_dir.glob(f"{rec_id}*.ogg"), None)
                best_file = mp3_file or ogg_file
                if best_file is None:
                    continue

                # Use relative path from audio_dir for HF to resolve
                audio_path = str(best_file.relative_to(audio_dir))

                records.append(
                    {
                        "recording_id": rec_id,
                        "audio_path": audio_path,
                        "term": term_dir.name,
                        "docket": docket_dir.name,
                    }
                )

    return records


def collect_recordings(
    audio_dir: Path,
    terms: list[str] | None,
    transcripts_dir: Path | None = None,
    speakers_dir: Path | None = None,
) -> list[dict[str, str | float | int | None | list[int]]]:
    """Collect recording metadata from processed audio (flex dataset).

    Edited by Claude: Added speaker metadata (justice_speakers, other_speakers, total_speakers).

    Args:
        audio_dir: Directory with processed audio files.
        terms: Optional list of terms to filter.
        transcripts_dir: Optional directory with transcript files for speaker metadata.
        speakers_dir: Optional directory with speaker files for justice detection.
    """
    records: list[dict[str, str | float | int | None | list[int]]] = []
    term_set = set(terms) if terms else None

    if not audio_dir.exists():
        return records

    # Load justice speaker IDs for classification
    justice_ids = load_justice_speaker_ids(speakers_dir) if speakers_dir else set()

    # Build transcript lookup: (term, docket, transcript_type) -> list of speaker IDs
    transcript_speakers: dict[tuple[str, str, str], list[int]] = {}
    if transcripts_dir and transcripts_dir.exists():
        for term_dir in transcripts_dir.iterdir():
            if not term_dir.is_dir():
                continue
            if term_set and term_dir.name not in term_set:
                continue

            for docket_dir in term_dir.iterdir():
                if not docket_dir.is_dir():
                    continue

                for transcript_file in docket_dir.glob("*.json"):
                    try:
                        with transcript_file.open() as f:
                            data = json.load(f)

                        term = data.get("term", term_dir.name)
                        docket = data.get("case_docket", docket_dir.name)
                        transcript_type = data.get("type", "unknown")
                        key = (term, docket, transcript_type)

                        # Extract unique speaker IDs from transcript
                        speaker_ids: set[int] = set()
                        for turn in data.get("turns", []):
                            if (
                                turn.get("is_valid")
                                and turn.get("speaker_id") is not None
                            ):
                                speaker_ids.add(turn["speaker_id"])

                        if speaker_ids:
                            transcript_speakers[key] = list(speaker_ids)
                    except (json.JSONDecodeError, KeyError):
                        continue

    for term_dir in audio_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            for meta_file in docket_dir.glob("*.metadata.json"):
                try:
                    with meta_file.open() as f:
                        meta = json.load(f)

                    flac_name = meta_file.stem.replace(".metadata", "") + ".flac"
                    flac_path = docket_dir / flac_name
                    recording_id = meta_file.stem.replace(".metadata", "")

                    # Only include recordings where FLAC file actually exists
                    # Edited by Claude: Validate FLAC exists to prevent skipped utterances
                    if not flac_path.exists():
                        continue

                    # Edited by Claude: Add transcript_type from recording_id
                    transcript_type = parse_transcript_type_from_recording_id(
                        recording_id
                    )

                    # Get speaker metadata for this recording
                    key = (term_dir.name, docket_dir.name, transcript_type)
                    recording_speaker_ids = transcript_speakers.get(key, [])
                    justice_speakers = [
                        sid for sid in recording_speaker_ids if sid in justice_ids
                    ]
                    other_speakers = [
                        sid for sid in recording_speaker_ids if sid not in justice_ids
                    ]
                    total_speakers = len(recording_speaker_ids)

                    records.append(
                        {
                            "term": term_dir.name,
                            "docket": docket_dir.name,
                            "recording_id": recording_id,
                            "transcript_type": transcript_type,
                            "audio_path": str(flac_path.relative_to(audio_dir)),
                            "duration_sec": meta.get("duration"),
                            "sample_rate": meta.get("sample_rate"),
                            "channels": meta.get("channels"),
                            "source_format": meta.get("source_format"),
                            "source_era": meta.get("source_era"),
                            "justice_speakers": justice_speakers,
                            "other_speakers": other_speakers,
                            "total_speakers": total_speakers,
                        }
                    )
                except (json.JSONDecodeError, KeyError):
                    continue

    return records


def collect_utterances(
    transcripts_dir: Path, terms: list[str] | None, speakers_dir: Path | None = None
) -> list[dict[str, str | float | int | None | bool]]:
    """Collect utterances from processed transcripts.

    Edited by Claude: Added is_justice field based on speaker files.

    Args:
        transcripts_dir: Directory with processed transcripts.
        terms: Optional list of terms to filter.
        speakers_dir: Optional directory with speaker files for justice detection.
    """
    utterances: list[dict[str, str | float | int | None | bool]] = []
    term_set = set(terms) if terms else None

    # Load justice speaker IDs
    justice_ids = load_justice_speaker_ids(speakers_dir)

    if not transcripts_dir.exists():
        return utterances

    for term_dir in transcripts_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            for transcript_file in docket_dir.glob("*.json"):
                try:
                    with transcript_file.open() as f:
                        data = json.load(f)

                    term = data.get("term", term_dir.name)
                    docket = data.get("case_docket", docket_dir.name)
                    transcript_type = data.get("type", "")

                    for turn in data.get("turns", []):
                        speaker_id = turn.get("speaker_id")
                        is_justice = (
                            speaker_id is not None and speaker_id in justice_ids
                        )

                        utterances.append(
                            {
                                "term": term,
                                "docket": docket,
                                "transcript_type": transcript_type,
                                "turn_index": turn.get("index"),
                                "start_sec": turn.get("start"),
                                "end_sec": turn.get("stop"),
                                "duration_sec": turn.get("duration"),
                                "speaker_id": speaker_id,
                                "speaker_name": turn.get("speaker_name"),
                                "is_justice": is_justice,
                                "text": turn.get("text"),
                                "word_count": turn.get("word_count"),
                                "valid": turn.get("is_valid", False),
                                "invalid_reason": turn.get("invalid_reason"),
                            }
                        )
                except (json.JSONDecodeError, KeyError):
                    continue

    return utterances


def collect_speakers(
    speakers_dir: Path, terms: list[str] | None
) -> list[dict[str, Any]]:
    """Collect speaker statistics from speaker JSON files.

    Edited by Claude: Aggregates speaker data for speakers mode.

    Args:
        speakers_dir: Directory containing speakers/justices/ and speakers/other/ subdirectories.
        terms: Optional list of terms to filter.

    Returns
    -------
        List of speaker dictionaries with aggregated statistics.
    """
    speakers: list[dict[str, Any]] = []
    term_set = set(terms) if terms else None

    if not speakers_dir.exists():
        return speakers

    # Load from both justices and other directories
    for subdir_name in ("justices", "other"):
        subdir = speakers_dir / subdir_name
        if not subdir.exists():
            continue

        for speaker_file in subdir.glob("*.json"):
            try:
                with speaker_file.open() as f:
                    data = json.load(f)

                # Filter by terms if specified
                if term_set:
                    by_term = data.get("by_term", {})
                    matching_terms = set(by_term.keys()) & term_set
                    if not matching_terms:
                        continue
                    # Recalculate totals for matching terms
                    totals = data.get("totals", {})
                    filtered_recordings = sum(
                        by_term.get(term, {}).get("recordings", 0)
                        for term in matching_terms
                    )
                    filtered_turns = sum(
                        by_term.get(term, {}).get("turns", 0) for term in matching_terms
                    )
                    filtered_duration = sum(
                        by_term.get(term, {}).get("duration_seconds", 0.0)
                        for term in matching_terms
                    )
                    filtered_words = sum(
                        by_term.get(term, {}).get("word_count", 0)
                        for term in matching_terms
                    )
                    filtered_cases = len(
                        {
                            case
                            for case in data.get("cases", [])
                            if any(
                                case.startswith(f"{term}/") for term in matching_terms
                            )
                        }
                    )

                    speaker_data = {
                        "speaker_id": data.get("id"),
                        "name": data.get("name"),
                        "role": data.get("role", "other"),
                        "total_recordings": filtered_recordings,
                        "total_cases": filtered_cases,
                        "total_turns": filtered_turns,
                        "total_duration_sec": round(filtered_duration, 2),
                        "total_word_count": filtered_words,
                        "first_appearance": min(
                            (term for term in matching_terms if term in by_term),
                            default=None,
                        ),
                        "last_appearance": max(
                            (term for term in matching_terms if term in by_term),
                            default=None,
                        ),
                        "by_term": {
                            k: v for k, v in by_term.items() if k in matching_terms
                        },
                    }
                else:
                    totals = data.get("totals", {})
                    speaker_data = {
                        "speaker_id": data.get("id"),
                        "name": data.get("name"),
                        "role": data.get("role", "other"),
                        "total_recordings": totals.get("recordings", 0),
                        "total_cases": totals.get("cases", 0),
                        "total_turns": totals.get("turns", 0),
                        "total_duration_sec": totals.get("duration_seconds", 0.0),
                        "total_word_count": totals.get("word_count", 0),
                        "first_appearance": data.get("first_appearance"),
                        "last_appearance": data.get("last_appearance"),
                        "by_term": data.get("by_term", {}),
                    }

                speakers.append(speaker_data)
            except (json.JSONDecodeError, KeyError):
                continue

    return speakers
