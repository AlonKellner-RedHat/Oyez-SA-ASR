# Generated by Claude
"""Helper functions for dataset creation commands."""

import json
import shutil
from pathlib import Path
from typing import Any

import typer
from rich.console import Console
from tqdm import tqdm

from .term_filter import filter_dirs

console = Console(force_terminal=True)


def require_pyarrow() -> tuple[Any, Any]:
    """Import and return pyarrow modules, or exit if not installed."""
    try:
        import pyarrow as pa_mod  # noqa: PLC0415
        import pyarrow.parquet as pq_mod  # noqa: PLC0415

        return pa_mod, pq_mod
    except ImportError:
        console.print("[red]Error:[/red] pyarrow not installed.")
        console.print("Run: uv add pyarrow")
        raise typer.Exit(1) from None


def copy_tree(src: Path, dst: Path, desc: str = "Copying") -> int:
    """Copy a directory tree, returning the number of files copied."""
    if not src.exists():
        return 0
    count = 0
    files = [f for f in src.rglob("*") if f.is_file()]
    for file in tqdm(files, desc=desc, unit="file"):
        dest = dst / file.relative_to(src)
        dest.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(file, dest)
        count += 1
    return count


def copy_raw_audio(cache_dir: Path, output_dir: Path, terms: list[str] | None) -> int:
    """Copy raw audio files from cache."""
    total = 0
    for fmt in ("mp3", "ogg"):
        audio_cache = cache_dir / "audio" / f"oyez.case-media.{fmt}" / "case_data"
        if not audio_cache.exists():
            continue
        for term_dir in filter_dirs(list(audio_cache.iterdir()), terms):
            if term_dir.is_dir():
                dst = output_dir / "audio" / term_dir.name
                total += copy_tree(term_dir, dst, f"Audio {fmt}/{term_dir.name}")
    return total


def matches_term(json_file: Path, term_set: set[str]) -> bool:
    """Check if a JSON file's term is in the term set."""
    try:
        with json_file.open() as f:
            data = json.load(f)
        return data.get("term") in term_set
    except (json.JSONDecodeError, KeyError):
        return False


def copy_raw_cases(cache_dir: Path, output_dir: Path, term_set: set[str] | None) -> int:
    """Copy raw case JSON files from cache."""
    cases_cache = cache_dir / "cases" / "api.oyez.org" / "raw"
    if not cases_cache.exists():
        return 0
    cases_out = output_dir / "cases"
    cases_out.mkdir(parents=True, exist_ok=True)
    count = 0
    for json_file in tqdm(list(cases_cache.glob("*.json")), desc="Cases", unit="file"):
        if term_set and not matches_term(json_file, term_set):
            continue
        shutil.copy2(json_file, cases_out / json_file.name)
        count += 1
    return count


def copy_raw_transcripts(
    cache_dir: Path, output_dir: Path, term_set: set[str] | None
) -> int:
    """Copy raw transcript JSON files from cache.

    Note: Raw transcripts are stored by media ID, not term. Term filtering
    requires parsing each file to extract term info, which is not currently
    supported. When term filtering is requested, all transcripts are copied.
    """
    if term_set:
        console.print(
            "  [yellow]Note:[/yellow] Transcript term filtering not supported; "
            "copying all transcripts"
        )
    transcripts_cache = cache_dir / "transcripts" / "api.oyez.org" / "raw"
    if not transcripts_cache.exists():
        return 0
    transcripts_out = output_dir / "transcripts"
    transcripts_out.mkdir(parents=True, exist_ok=True)
    count = 0
    for json_file in tqdm(
        list(transcripts_cache.glob("*.json")), desc="Transcripts", unit="file"
    ):
        shutil.copy2(json_file, transcripts_out / json_file.name)
        count += 1
    return count


def collect_raw_recordings(
    audio_dir: Path, terms: list[str] | None
) -> list[dict[str, str | None]]:
    """Collect raw recording metadata from MP3/OGG files for HuggingFace parquet.

    Creates a parquet with audio paths that HuggingFace can resolve.
    """
    records: list[dict[str, str | None]] = []
    term_set = set(terms) if terms else None

    if not audio_dir.exists():
        return records

    # Track unique recording IDs
    seen: set[str] = set()

    for term_dir in audio_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            # Collect both MP3 and OGG files
            for audio_file in docket_dir.glob("*"):
                if audio_file.suffix not in (".mp3", ".ogg"):
                    continue

                # Extract recording ID (handle .delivery.mp3 pattern)
                rec_id = audio_file.stem.split(".")[0]
                if rec_id in seen:
                    continue
                seen.add(rec_id)

                # Find best audio file (prefer MP3)
                mp3_file = next(docket_dir.glob(f"{rec_id}*.mp3"), None)
                ogg_file = next(docket_dir.glob(f"{rec_id}*.ogg"), None)
                best_file = mp3_file or ogg_file
                if best_file is None:
                    continue

                # Use relative path from audio_dir for HF to resolve
                audio_path = str(best_file.relative_to(audio_dir))

                records.append(
                    {
                        "recording_id": rec_id,
                        "audio_path": audio_path,
                        "term": term_dir.name,
                        "docket": docket_dir.name,
                    }
                )

    return records


def collect_recordings(
    audio_dir: Path, terms: list[str] | None
) -> list[dict[str, str | float | int | None]]:
    """Collect recording metadata from processed audio (flex dataset)."""
    records: list[dict[str, str | float | int | None]] = []
    term_set = set(terms) if terms else None

    if not audio_dir.exists():
        return records

    for term_dir in audio_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            for meta_file in docket_dir.glob("*.metadata.json"):
                try:
                    with meta_file.open() as f:
                        meta = json.load(f)

                    flac_name = meta_file.stem.replace(".metadata", "") + ".flac"
                    flac_path = docket_dir / flac_name

                    records.append(
                        {
                            "term": term_dir.name,
                            "docket": docket_dir.name,
                            "recording_id": meta_file.stem.replace(".metadata", ""),
                            "audio_path": str(flac_path.relative_to(audio_dir)),
                            "duration_sec": meta.get("duration"),
                            "sample_rate": meta.get("sample_rate"),
                            "channels": meta.get("channels"),
                            "source_format": meta.get("source_format"),
                            "source_era": meta.get("source_era"),
                        }
                    )
                except (json.JSONDecodeError, KeyError):
                    continue

    return records


def collect_utterances(
    transcripts_dir: Path, terms: list[str] | None
) -> list[dict[str, str | float | int | None]]:
    """Collect utterances from processed transcripts."""
    utterances: list[dict[str, str | float | int | None]] = []
    term_set = set(terms) if terms else None

    if not transcripts_dir.exists():
        return utterances

    for term_dir in transcripts_dir.iterdir():
        if not term_dir.is_dir():
            continue
        if term_set and term_dir.name not in term_set:
            continue

        for docket_dir in term_dir.iterdir():
            if not docket_dir.is_dir():
                continue

            for transcript_file in docket_dir.glob("*.json"):
                try:
                    with transcript_file.open() as f:
                        data = json.load(f)

                    term = data.get("term", term_dir.name)
                    docket = data.get("case_docket", docket_dir.name)
                    transcript_type = data.get("type", "")

                    for turn in data.get("turns", []):
                        utterances.append(
                            {
                                "term": term,
                                "docket": docket,
                                "transcript_type": transcript_type,
                                "turn_index": turn.get("index"),
                                "start_sec": turn.get("start"),
                                "end_sec": turn.get("stop"),
                                "duration_sec": turn.get("duration"),
                                "speaker_id": turn.get("speaker_id"),
                                "speaker_name": turn.get("speaker_name"),
                                "text": turn.get("text"),
                                "word_count": turn.get("word_count"),
                                "valid": turn.get("is_valid", False),
                                "invalid_reason": turn.get("invalid_reason"),
                            }
                        )
                except (json.JSONDecodeError, KeyError):
                    continue

    return utterances
