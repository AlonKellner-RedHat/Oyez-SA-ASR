# Generated by Claude
"""HuggingFace loading script for oyez-flex dataset.

For datasets v3.x with trust_remote_code=True:
    # Load full recordings
    ds = load_dataset("datasets/flex", "recordings", trust_remote_code=True)

    # Load utterances with on-the-fly segment extraction
    ds = load_dataset("datasets/flex", "utterances", trust_remote_code=True)

For datasets v4.x, use parquet auto-discovery instead (full files only).
"""

import io
from collections.abc import Iterator
from pathlib import Path
from typing import Any, ClassVar

import av
import numpy as np
import pyarrow.parquet as pq

import datasets

_DESCRIPTION = """\
Oyez Supreme Court Oral Arguments - Flex Dataset.
Processed FLAC audio with parquet metadata for flexible access.
"""

_HOMEPAGE = "https://www.oyez.org/"
_LICENSE = "CC-BY-4.0"


class OyezFlex(datasets.GeneratorBasedBuilder):
    """Oyez flex dataset with processed FLAC audio."""

    VERSION = datasets.Version("1.0.0")

    BUILDER_CONFIGS: ClassVar[list[datasets.BuilderConfig]] = [
        datasets.BuilderConfig(
            name="recordings",
            description="Full recordings with FLAC audio",
        ),
        datasets.BuilderConfig(
            name="utterances",
            description="Utterance-level segments with extracted audio",
        ),
    ]

    DEFAULT_CONFIG_NAME = "recordings"

    def _info(self) -> datasets.DatasetInfo:
        """Define dataset schema based on config."""
        if self.config.name == "recordings":
            features = datasets.Features(
                {
                    "recording_id": datasets.Value("string"),
                    "audio": datasets.Audio(sampling_rate=22050),
                    "term": datasets.Value("string"),
                    "docket": datasets.Value("string"),
                    "duration_sec": datasets.Value("float64"),
                    "sample_rate": datasets.Value("int64"),
                    "channels": datasets.Value("int64"),
                    "source_format": datasets.Value("string"),
                    "source_era": datasets.Value("string"),
                }
            )
        else:  # utterances
            features = datasets.Features(
                {
                    "id": datasets.Value("string"),
                    "audio": datasets.Audio(sampling_rate=22050),
                    "text": datasets.Value("string"),
                    "speaker_name": datasets.Value("string"),
                    "start_sec": datasets.Value("float64"),
                    "end_sec": datasets.Value("float64"),
                    "duration_sec": datasets.Value("float64"),
                    "term": datasets.Value("string"),
                    "docket": datasets.Value("string"),
                }
            )

        return datasets.DatasetInfo(
            description=_DESCRIPTION,
            features=features,
            homepage=_HOMEPAGE,
            license=_LICENSE,
        )

    def _split_generators(  # type: ignore[override]
        self,
        dl_manager: datasets.DownloadManager  # noqa: ARG002
        | datasets.StreamingDownloadManager,
    ) -> list[datasets.SplitGenerator]:
        """Define single train split."""
        return [
            datasets.SplitGenerator(
                name="train",
                gen_kwargs={},
            )
        ]

    def _generate_examples(  # type: ignore[override]
        self,
        **kwargs: Any,  # noqa: ARG002
    ) -> Iterator[tuple[int, dict[str, Any]]]:
        """Yield examples based on config."""
        base = Path(self.config.data_dir) if self.config.data_dir else Path(".")
        audio_dir = base / "audio"
        data_dir = base / "data"

        if self.config.name == "recordings":
            yield from self._generate_recordings(data_dir, audio_dir)
        else:
            yield from self._generate_utterances(data_dir, audio_dir)

    def _generate_recordings(
        self, data_dir: Path, audio_dir: Path
    ) -> Iterator[tuple[int, dict[str, Any]]]:
        """Yield full recording examples."""
        recordings_pq = data_dir / "recordings.parquet"
        if not recordings_pq.exists():
            return

        table = pq.read_table(recordings_pq)
        for idx, row in enumerate(table.to_pylist()):
            audio_path = audio_dir / row["audio_path"]
            if not audio_path.exists():
                continue

            yield (
                idx,
                {
                    "recording_id": row["recording_id"],
                    "audio": str(audio_path),
                    "term": row["term"],
                    "docket": row["docket"],
                    "duration_sec": row["duration_sec"],
                    "sample_rate": row["sample_rate"],
                    "channels": row["channels"],
                    "source_format": row["source_format"],
                    "source_era": row["source_era"],
                },
            )

    def _generate_utterances(  # noqa: PLR0912, PLR0915
        self, data_dir: Path, audio_dir: Path
    ) -> Iterator[tuple[int, dict[str, Any]]]:
        """Yield utterance examples with on-the-fly segment extraction."""
        recordings_pq = data_dir / "recordings.parquet"
        utterances_pq = data_dir / "utterances.parquet"

        if not recordings_pq.exists() or not utterances_pq.exists():
            return

        # Build recording lookup
        rec_table = pq.read_table(recordings_pq)
        rec_lookup = {}
        for row in rec_table.to_pylist():
            key = (row["term"], row["docket"])
            if key not in rec_lookup:
                rec_lookup[key] = []
            rec_lookup[key].append(row)

        # Cache for loaded audio files
        audio_cache: dict[str, tuple[np.ndarray, int]] = {}

        utt_table = pq.read_table(utterances_pq)
        idx = 0

        for row in utt_table.to_pylist():
            if not row.get("valid", True):
                continue

            key = (row["term"], row["docket"])
            recs = rec_lookup.get(key, [])
            if not recs:
                continue

            # Use first matching recording
            rec = recs[0]
            audio_path = audio_dir / rec["audio_path"]
            if not audio_path.exists():
                continue

            # Load audio if not cached
            cache_key = str(audio_path)
            if cache_key not in audio_cache:
                try:
                    container = av.open(str(audio_path))
                    stream = container.streams.audio[0]
                    sample_rate = stream.rate
                    frames = []
                    for frame in container.decode(audio=0):
                        frames.append(frame.to_ndarray())
                    container.close()
                    audio_data = np.concatenate(frames, axis=1).flatten()
                    audio_cache[cache_key] = (audio_data, sample_rate)
                except Exception:  # noqa: S112
                    continue

            audio_data, sample_rate = audio_cache[cache_key]

            # Extract segment
            start_sample = int(row["start_sec"] * sample_rate)
            end_sample = int(row["end_sec"] * sample_rate)
            end_sample = min(end_sample, len(audio_data))
            if start_sample >= end_sample:
                continue

            segment = audio_data[start_sample:end_sample]

            # Encode segment as WAV bytes for HF Audio feature
            output = io.BytesIO()
            out_container = av.open(output, mode="w", format="wav")
            out_stream: av.AudioStream = out_container.add_stream(  # type: ignore[assignment]
                "pcm_f32le", rate=sample_rate
            )
            out_stream.layout = "mono"  # type: ignore[assignment]
            frame = av.AudioFrame.from_ndarray(
                segment.reshape(1, -1).astype(np.float32), format="flt", layout="mono"
            )
            frame.rate = sample_rate
            for packet in out_stream.encode(frame):
                out_container.mux(packet)
            for packet in out_stream.encode():
                out_container.mux(packet)
            out_container.close()

            utt_id = f"{row['term']}_{row['docket']}_{row['turn_index']}"

            yield (
                idx,
                {
                    "id": utt_id,
                    "audio": {"bytes": output.getvalue(), "path": f"{utt_id}.wav"},
                    "text": row.get("text", ""),
                    "speaker_name": row.get("speaker_name", ""),
                    "start_sec": row["start_sec"],
                    "end_sec": row["end_sec"],
                    "duration_sec": row["duration_sec"],
                    "term": row["term"],
                    "docket": row["docket"],
                },
            )
            idx += 1
